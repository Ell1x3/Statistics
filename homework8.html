<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Homework 9 - Statistical Analogies and Mathematical Properties - Gaia Diodati</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
      MathJax = {
          tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']] 
          }
      };
  </script>
  <style>
    /* Stili CSS ripresi e adattati */
    @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&family=Rajdhani:wght@400;600&display=swap');

    body {
      margin: 0;
      font-family: 'Rajdhani', sans-serif;
      color: #fff5cc;
      background: #050505;
      overflow-x: hidden;
    }
    
    canvas#bg {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: -1;
    }

    .hero {
      min-height: 50vh; 
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      padding: 0 4vw;
    }

    h1 {
      font-family: 'Orbitron', sans-serif;
      font-size: clamp(2.2rem, 8vw, 5rem);
      letter-spacing: 3px;
      text-transform: uppercase;
      color: #e6b800;
      position: relative;
      text-shadow: 0 0 8px #ffcc00, 0 0 16px #ffaa00, 2px 2px 35px rgba(255,200,0,0.8);
      animation: glowPulse 4s ease-in-out infinite, glitch 2s infinite;
      margin: 0 0 12px;
    }

    h1::before, h1::after {
      content: attr(data-text);
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      color: #ffaa00;
      background: transparent;
      overflow: hidden;
    }

    h1::before {
      left: 2px;
      text-shadow: -2px 0 #ff33cc;
      animation: glitchShift 2s infinite linear alternate-reverse;
    }

    h1::after {
      left: -2px;
      text-shadow: 2px 0 #00ffcc;
      animation: glitchShift 3s infinite linear alternate;
    }

    h2 {
      font-size: clamp(1.2rem, 4vw, 2rem);
      color: #ffd966;
      margin: 6px 0;
    }

    .subtitle {
      font-size: clamp(0.9rem, 3vw, 1rem);
      color: #fff3cc;
      opacity: 0.9;
      line-height: 1.5;
    }
    
    @keyframes glitchShift {
      0% { clip-path: inset(0 0 90% 0); }
      20% { clip-path: inset(10% 0 70% 0); }
      40% { clip-path: inset(40% 0 30% 0); }
      60% { clip-path: inset(60% 0 10% 0); }
      80% { clip-path: inset(80% 0 0 0); }
      100% { clip-path: inset(0 0 100% 0); }
    }

    @keyframes glowPulse {
      0%, 100% { text-shadow: 0 0 8px #ffcc00, 0 0 20px #ffaa00, 0 0 40px #ffcc33; }
      50% { text-shadow: 0 0 15px #ffee88, 0 0 35px #ffaa00, 0 0 60px #ffd966; }
    }

    @keyframes glitch {
      0%, 100% { transform: none; }
      92% { transform: translate(-1px, 1px) skew(0.3deg); }
      94% { transform: translate(1px, -1px) skew(-0.3deg); }
      96% { transform: translate(-2px, 1px) skew(0.5deg); }
      98% { transform: translate(2px, -1px) skew(-0.5deg); }
    }


    /* Stili per il contenuto */
    .content-section {
        padding: 50px 5vw 70px;
        background: rgba(20, 20, 20, 0.8);
        display: flex;
        flex-direction: column;
        align-items: center;
    }

    .post-card {
        width: 90%;
        max-width: 900px; 
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 200, 0, 0.3);
        border-radius: 12px;
        padding: 22px;
        text-align: left;
        backdrop-filter: blur(6px);
        margin-bottom: 25px;
    }
    
    .post-card h3 {
        font-family: 'Orbitron', sans-serif;
        color: #ffd966;
        margin-top: 25px;
        margin-bottom: 10px;
        border-bottom: 1px solid rgba(255, 200, 0, 0.2);
        padding-bottom: 5px;
        font-size: clamp(1.1rem, 3.5vw, 1.4rem);
    }
    .post-card p, .post-card ul, .post-card ol {
        color: #fff3cc;
        font-size: clamp(0.9rem, 2.8vw, 1rem);
        line-height: 1.6;
        margin-bottom: 15px;
    }
    
    .post-card pre {
        background: rgba(0, 0, 0, 0.5);
        border: 1px solid rgba(255, 200, 0, 0.1);
        padding: 15px;
        border-radius: 8px;
        overflow-x: auto;
        white-space: pre-wrap;
        color: #00ffcc; 
        font-family: monospace;
    }

  </style>
</head>
<body>
  <canvas id="bg"></canvas>

  <div class="hero">
    <h1 data-text="Homework 9">Homework 9</h1>
    <h2>Statistical Analogies and Mathematical Properties</h2>
    <p class="subtitle">Gaia Diodati ‚Äì Statistics Course</p>
  </div>

  <section class="content-section">
    <div class="post-card">
      <h2>üîó Analogies between Random Walk (HW8) and Law of Large Numbers (HW4)</h2>
      <p>
        Both the <b><i>Random Walk</i></b> simulation (HW8) and the <b><i>Law of Large Numbers</i></b> (LLN) simulation (HW4) are founded on the same fundamental building block: the <i>Bernoulli Process</i>. They both examine the long-term behavior of a sequence of independent trials, but they focus on different aspects and use different variables.
      </p>

      <h3>Similarities: Bernoulli Process Foundation</h3>
      <ul>
        <li>
          <b><i>Independent Trials</i></b>: In both homeworks, each step (either a single trial $n$ in LLN or a week $n$ in the Random Walk) is independent of the previous ones.
        </li>
        <li>
          <b><i>Fixed Probability</i></b>: The underlying probability of "success" ($p$) remains constant in both processes. In HW8, the compound probability of the server being secure ($p_{rw}$) is fixed for all weeks.
        </li>
        <li>
          <b><i>Binary Outcomes</i></b>: Both models use binary outcomes: Success/Failure (LLN) and Secure (+1)/Breached (-1) (HW8).
        </li>
      </ul>

      <h3>Key Differences: Focus and Convergence</h3>
      <table style="width: 100%; border-collapse: collapse; margin-top: 20px;">
        <thead>
          <tr>
            <th style="border: 1px solid #ffaa00; padding: 8px; color: #ffaa00;">Feature</th>
            <th style="border: 1px solid #ffaa00; padding: 8px; color: #ffaa00;">HW4 (LLN / Frequency)</th>
            <th style="border: 1px solid #ffaa00; padding: 8px; color: #ffaa00;">HW8 (Random Walk / Score)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border: 1px solid #ffaa00; padding: 8px;"><b><i>Observed Variable</i></b></td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Relative Frequency: $f_n = K_n / n$</td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Cumulative Score: $S_n = \sum (X_i)$</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ffaa00; padding: 8px;"><b><i>Value Range</i></b></td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Continuous (0 to 1)</td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Discrete (from $-n$ to $+n$)</td>
          </tr>
          <tr>
            <td style="border: 1px solid #ffaa00; padding: 8px;"><b><i>Convergence Goal</i></b></td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Convergence of $f_n$ to the fixed probability $p$ (Weak LLN).</td>
            <td style="border: 1px solid #ffaa00; padding: 8px;">Distribution of $S_n$ converges to the <b><i>Binomial Distribution</i></b>.</td>
          </tr>
        </tbody>
      </table>
      <p style="margin-top: 15px;">
        While the LLN focuses on <i>averaging</i> out the randomness over time ($f_n \to p$), the Random Walk focuses on the <i>sum</i> of the outcomes, whose distribution is perfectly described by the <i>Binomial theorem</i>.
      </p>
    </div>

    <div class="post-card">
      <h2>üìê Formalization: $S_n$ as a Linear Transformation of $K_n$</h2>
      <p>
        The observed convergence to the <i>Binomial Distribution</i> in HW8 is not coincidental, but a <b><i>direct mathematical consequence</i></b> of how the score variable is defined. The final score (position of the Random Walk, $S_n$) is simply a linear transformation of the core <i>Binomial variable</i> (the number of successes, $K_n$).
      </p>

      <h3>Defining the Variables</h3>
      <ul>
        <li>
            Let $K_n$ be the <i>Binomial Variable</i>, the count of successful outcomes ($+1$ score, secure weeks) over $n$ trials. $K_n \sim \text{Binomiale}(n, p_{rw})$.
        </li>
        <li>
            Let $S_n$ be the <i>Random Walk Score</i>, the final cumulative position.
        </li>
      </ul>

      <h3>The Formal Relationship</h3>
      <p>
        For every trajectory, if there are $K_n$ successes, there must be $(n - K_n)$ failures (scores of $-1$). The total cumulative score $S_n$ is therefore calculated as:
      </p>
      <p style="text-align: center; font-family: 'Orbitron', sans-serif; font-size: 1.4rem; color: #00ffcc;">
        $$S_n = K_n (+1) + (n - K_n) (-1)$$
      </p>
      <p>
        Simplifying this expression yields the fundamental linear transformation:
      </p>
      <p style="text-align: center; font-family: 'Orbitron', sans-serif; font-size: 1.6rem; color: #00ffcc;">
        $$S_n = 2K_n - n$$
      </p>
      <p>
        This confirms that the distribution of the final score $S_n$ is a <i>scaled and translated version</i> of the Binomial distribution of $K_n$. Observing the convergence of $S_n$ to the theoretical Binomial PMF is therefore a <i>robust test of the statistical model's integrity</i>.
      </p>
    </div>

    <div class="post-card">
      <h2>‚öõÔ∏è Mathematical Properties and Asymptotic Convergence (CLT)</h2>
      <p>
        The underlying mathematical structure links the <i>Bernoulli process</i> not only to discrete combinatorics but also to the most important theorem in continuous probability: the <i>Central Limit Theorem</i>.
      </p>

      <h3>1. Binomial Coefficients and Combinatorics</h3>
      <p>
        The probability of achieving exactly <b><i>k</i></b> successes in <b><i>n</i></b> trials is given by the Binomial Probability Mass Function (PMF):
        $$P(K=k) = \binom{n}{k} p^k (1-p)^{n-k}$$
        The term $\binom{n}{k}$, the <i>Binomial Coefficient</i>, determines the number of paths (combinations) that lead to $k$ successes. These coefficients form <i>Pascal's Triangle</i>:
      </p>
      <pre><code>
  n=0:       1
  n=1:      1 1
  n=2:     1 2 1
  n=3:    1 3 3 1
  n=4:   1 4 6 4 1
        
        </code></pre>
      <p>
        The Binomial PMF is mathematically derived from the <i>Binomial Theorem</i> (or <i>Binomial Expansion</i>), where the sum of all probabilities equals $1$:
        $$\sum_{k=0}^n P(K=k) = ((1-p) + p)^n = 1^n = 1$$
      </p>

      <h3>2. The Connection to Fibonacci Sequence</h3>
      <p>
        The <i>Fibonacci sequence</i> ($F_n$: 1, 1, 2, 3, 5, 8, ...) is linked to <i>Pascal's Triangle</i> through its <i>shallow diagonals</i>. The sum of the coefficients along these diagonals generates the Fibonacci numbers. This highlights the rich interconnectedness of combinatorial mathematics:
      </p>
      <pre><code>
F_n = sum of coefficients along the shallow diagonals of Pascal's Triangle.
e.g., F_5 = 1 + 3 + 1 = 5
        </code></pre>

      <h3>3. Asymptotic Convergence: The Central Limit Theorem (CLT)</h3>
      <p>
        The relationship between HW4 (LLN) and HW8 (Random Walk) is completed by the <i>Central Limit Theorem (CLT)</i>.
      </p>
      <ul>
        <li>
            The LLN (HW4) guarantees that the sample average ($\bar{X}_n$) converges to the true mean $\mu$.
        </li>
        <li>
            The CLT explains the <i>shape</i> of the distribution when $n$ is large. Since the Random Walk score ($S_n$) is the sum of $n$ independent and identically distributed (i.i.d.) random variables, the CLT dictates that the distribution of the standardized sum tends towards the <i>Standard Normal Distribution</i> ($\mathcal{N}(0, 1)$).
        </li>
      </ul>
      <p>
        Thus, the 'bell shape' observed in the simulated histograms of HW8 for large $n$ is the visual manifestation of the CLT, confirming that the <i>Random Walk</i> is <i>asymptotically Normal</i>. This completes the theoretical bridge between the basic <i>Bernoulli process</i> and the continuous <i>Gaussian distribution</i>. 
      </p>
    </div>
  </section>


  <script>
    // --- CANVAS BACKGROUND SCRIPT (Incluso per completezza e stile) ---
    const canvas = document.getElementById('bg');
    const ctx = canvas.getContext('2d');

    const points = [];
    const numPoints = 100;

    function initPoints() {
      points.length = 0; 
      for (let i = 0; i < numPoints; i++) {
        points.push({
          x: Math.random() * canvas.width,
          y: Math.random() * canvas.height,
          vx: (Math.random() - 0.5) * 0.6,
          vy: (Math.random() - 0.5) * 0.6
        });
      }
    }

    function resizeCanvas() {
      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;
      initPoints();
    }
    
    resizeCanvas(); 
    window.addEventListener('resize', resizeCanvas);


    function draw() {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      for (let i = 0; i < points.length; i++) {
        const p1 = points[i];
        for (let j = i + 1; j < points.length; j++) {
          const p2 = points[j];
          const dx = p1.x - p2.x;
          const dy = p1.y - p2.y;
          const dist = Math.sqrt(dx * dx + dy * dy);
          if (dist < 130) {
            ctx.strokeStyle = `rgba(255, 200, 0, ${(1 - dist / 130) * 0.15})`;
            ctx.beginPath();
            ctx.moveTo(p1.x, p1.y);
            ctx.lineTo(p2.x, p2.y);
            ctx.stroke();
          }
        }

        p1.x += p1.vx;
        p1.y += p1.vy;
        
        if (p1.x < 0 || p1.x > canvas.width) p1.vx *= -1;
        if (p1.y < 0 || p1.y > canvas.height) p1.vy *= -1;

        ctx.beginPath();
        ctx.arc(p1.x, p1.y, 2, 0, Math.PI * 2);
        ctx.fillStyle = '#ffcc00';
        ctx.fill();
      }
      requestAnimationFrame(draw);
    }
    draw();
  </script>
</body>
</html>